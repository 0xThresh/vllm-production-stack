# LLMStack Tutorials

Welcome to the tutorials for LLMStack! This series of tutorials is designed to guide you through setting up and utilizing the vLLM production stack efficiently. Whether you're new to Kubernetes, Helm, or vLLM, or looking to deepen your understanding of advanced features like multi-model management and KV cache offloading, this series has you covered.

## Table of Contents

1. [Install Kubernetes Environment](00-install-kubernetes-env.md)  
   Learn how to set up a Kubernetes environment as the foundation for your vLLM deployment.

2. [Minimal Helm Installation](01-minimal-helm-installation.md)  
   A step-by-step guide for deploying vLLM using Helm with minimal configuration.

3. [Basic vLLM Configuration](02-basic-vllm-config.md)  
   Configure vLLM for basic usage, covering the essential parameters and settings.

4. [Load Model from Persistent Volume](03-load-model-from-pv.md)  
   Discover how to load models from a persistent volume to ensure efficient resource usage.

5. [Launch Multiple Models](04-launch-multiple-model.md)  
   Learn how to deploy and manage multiple models simultaneously in your vLLM environment.

6. [Offload KV Cache](05-offload-kv-cache.md)  
   Understand how to offload the KV cache to optimize memory usage for large-scale deployments.

---

## Getting Started

These tutorials are designed to be followed sequentially for beginners, but you can also jump to a specific tutorial based on your needs. Each tutorial includes:
- Prerequisites
- Detailed steps
- Commands to execute
- Expected outputs
- Explanations to enhance your understanding

---

## Feedback and Contributions

If you encounter any issues or have suggestions for improving these tutorials, feel free to contribute by opening a pull request or an issue on our [GitHub repository](https://github.com/vllm-project/production-stack).

Happy learning!

