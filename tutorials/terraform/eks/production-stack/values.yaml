servingEngineSpec:
  runtimeClassName: ""

  labels:
    app: "vllm-inference"

  modelSpec:
  - name: "opt125m"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "facebook/opt-125m"

    replicaCount: 1

    requestCPU: 1
    requestMemory: "4Gi"
    requestGPU: 1

    pvcStorage: "10Gi"
    pvcAccessMode:
      - ReadWriteOnce
    storageClass: "gp2"

    nodeSelectorTerms:
      - matchExpressions:
        - key: nvidia.com/gpu
          operator: "In"
          values:
          - "present"
        - key: app
          operator: "In"
          values:
          - "vllm-inference"

routerSpec:
  enableRouter: true

  replicas: 1
  resources:
    requests:
      cpu: 1
      memory: 4G
    limits:
      cpu: 2
      memory: 8G

  containerPort: 8000
  servicePort: 80


  ingress:
    # Builds the ingress resource on the router service
    enabled: true
    className: "alb"

    # AWS ALB annotations to auto-deploy ALB when router ingress is created
    annotations:
      alb.ingress.kubernetes.io/load-balancer-name: "vllm-router-alb"
      alb.ingress.kubernetes.io/healthcheck-path: "/health"
      alb.ingress.kubernetes.io/success-codes: "200"
      alb.ingress.kubernetes.io/ip-address-type: "ipv4"
      alb.ingress.kubernetes.io/target-type: "ip"
      alb.ingress.kubernetes.io/scheme: "internet-facing"
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}]'
      kubernetes.io/ingress.class: "alb"
