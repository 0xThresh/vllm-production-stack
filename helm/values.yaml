# -- Default values for llmstack helm chart
# -- Declare variables to be passed into your templates.


# -- Serving engine configuratoon
servingEngineSpec:
  # -- Customized labels for the serving engine deployment
  labels:
    environment: "test"
    release: "test"

  # modelSpec - configuring multiple serving engines deployments that runs different models
  # Each entry in the modelSpec array should contain the following fields:
  # - name: (string) The name of the model, e.g., "example-model"
  # - repository: (string) The repository of the model, e.g., "vllm/vllm-openai"
  # - tag: (string) The tag of the model, e.g., "latest"
  # - modelURL: (string) The URL of the model, e.g., "facebook/opt-125m"
  #
  # - replicaCount: (int) The number of replicas for the model, e.g. 1
  # - requestCPU: (int) The number of CPUs requested for the model, e.g. 6
  # - requestMemory: (string) The amount of memory requested for the model, e.g., "16Gi"
  # - requestGPU: (int) The number of GPUs requested for the model, e.g., 1
  #
  # - pvcStorage: (string) The amount of storage requested for the model, e.g., "50Gi"
  # - pvcMatchLabels: (optional, map) The labels to match the PVC, e.g., {model: "opt125m"}
  #
  # - vllmConfig: (optional, map) The configuration for the VLLM model, supported options are:
  #   - enablePrefixCaching: (optional, bool) Enable prefix caching, e.g., false
  #   - enableChunkedPrefill: (optional, bool) Enable chunked prefill, e.g., false
  #   - maxModelLen: (optional, int) The maximum model length, e.g., 16384
  #   - dtype: (optional, string) The data type, e.g., "bfloat16"
  #   - extraArgs: (optional, list) Extra command line arguments to pass to vLLM, e.g., ["--disable-log-requests"]
  #
  # - lmcacheConfig: (optional, map) The configuration of the LMCache for KV offloading, supported options are:
  #   - enabled: (optional, bool) Enable LMCache, e.g., true
  #   - cpuOffloadingBufferSize: (optional, string) The CPU offloading buffer size, e.g., "30"
  #
  # - hf_token: (optional, string) the Huggingface tokens for this model
  #
  # - env: (optional, list) The environment variables to set in the container, e.g., your HF_TOKEN
  #
  # - nodeSelectorTerms: (optional, list) The node selector terms to match the nodes
  #
  # Example:
  # modelSpec:
  # - name: "mistral"
  #   repository: "lmcache/vllm-openai"
  #   tag: "latest"
  #   modelURL: "mistralai/Mistral-7B-Instruct-v0.2"
  #   replicaCount: 1
  #
  #   requestCPU: 10
  #   requestMemory: "64Gi"
  #   requestGPU: 1 
  #
  #   pvcStorage: "50Gi"
  #   pvcMatchLabels: 
  #     model: "mistral"
  #
  #   vllmConfig: 
  #     enableChunkedPrefill: false
  #     enablePrefixCaching: false
  #     maxModelLen: 16384
  #     dtype: "bfloat16"
  #     extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]
  #
  #   lmcacheConfig:
  #     enabled: true
  #     cpuOffloadingBufferSize: "30"
  #
  #   env:
  #     - name: HF_TOKEN
  #       value: <HUGGING_FACE_TOKEN>
  #
  #   nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nvidia.com/gpu.product
  #         operator: "In"
  #         values:
  #         - "NVIDIA-RTX-A6000"
  modelSpec: []

  # -- Container port 
  containerPort: 8000
  # -- Service port 
  servicePort: 80
  
  # -- Set other environment variables from config map
  configs: {}
  
  # -- Readiness probe configuration
  startupProbe:
    # -- Number of seconds after the container has started before startup probe is initiated
    initialDelaySeconds: 15
    # -- How often (in seconds) to perform the startup probe
    periodSeconds: 10
    # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready
    failureThreshold: 60
     # -- Configuration of the Kubelet http request on the server
    httpGet:
      # -- Path to access on the HTTP server
      path: /health
      # -- Name or number of the port to access on the container, on which the server is listening
      port: 8000
  
  # -- Liveness probe configuration
  livenessProbe:
   # -- Number of seconds after the container has started before liveness probe is initiated
    initialDelaySeconds: 15
    # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not alive
    failureThreshold: 3
    # -- How often (in seconds) to perform the liveness probe
    periodSeconds: 10
    # -- Configuration of the Kubelet http request on the server
    httpGet:
      # -- Path to access on the HTTP server
      path: /health
      # -- Name or number of the port to access on the container, on which the server is listening
      port: 8000
  
  # -- Disruption Budget Configuration
  maxUnavailablePodDisruptionBudget: ""

  # -- Tolerations configuration (when there are taints on nodes)
  # Example:
  # tolerations:
  #   - key: "node-role.kubernetes.io/control-plane"
  #     operator: "Exists"
  #     effect: "NoSchedule"
  tolerations: []

  # -- RuntimeClassName configuration, set to "nvidia" if the model requires GPU
  runtimeClassName: ""

routerSpec:
  # -- Number of replicas
  replicaCount: 1
  
  # -- Container port
  containerPort: 8000

  # -- Service port
  servicePort: 80

  # -- routing logic, could be "roundrobin" or "session"
  routingLogic: "roundrobin"

  # -- session key if using "session" routing logic
  sessionKey: ""

  # -- extra router commandline arguments
  extraArgs: []

  # -- Interval in seconds to scrape the serving engine metrics
  engineScrapeInterval: 15

  # -- Window size in seconds to calculate the request statistics
  requestStatsWindow: 60

  # -- Customized labels for the router deployment
  labels:
    environment: "router"
    release: "router"

  # -- TODO: Readiness probe configuration
  #startupProbe:
  #  # -- Number of seconds after the container has started before startup probe is initiated
  #  initialDelaySeconds: 5
  #  # -- How often (in seconds) to perform the startup probe
  #  periodSeconds: 5
  #  # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready
  #  failureThreshold: 100
  #   # -- Configuration of the Kubelet http request on the server
  #  httpGet:
  #    # -- Path to access on the HTTP server
  #
  # -- TODO: resource configuration: currently it uses 4 CPUs and 16Gi memory by default


prometheus:
  # Determines whether Prometheus is enabled as part of installing vLLM chart
  enabled: true 

  ## Create default rules for monitoring the cluster
  #
  # Disable `etcd` and `kubeScheduler` rules (managed by DOKS, so metrics are not accesible)
  defaultRules:
    create: true
    rules:
      etcd: false
      kubeScheduler: false

  ## Component scraping kube scheduler
  ##
  # Disabled because it's being managed by DOKS, so it's not accessible
  kubeScheduler:
    enabled: false

  ## Component scraping etcd
  ##
  # Disabled because it's being managed by DOKS, so it's not accessible
  kubeEtcd:
    enabled: false

  alertmanager:
    ## Deploy alertmanager
    ##
    enabled: true
    # config:
    #   global:
    #     resolve_timeout: 5m
    #     slack_api_url: "<YOUR_SLACK_APP_INCOMING_WEBHOOK_URL_HERE>"
    #   route:
    #     receiver: "slack-notifications"
    #     repeat_interval: 12h
    #     routes:
    #       - receiver: "slack-notifications"
    #         # matchers:
    #         #   - alertname="EmojivotoInstanceDown"
    #         # continue: false
    #   receivers:
    #     - name: "slack-notifications"
    #       slack_configs:
    #         - channel: "#<YOUR_SLACK_CHANNEL_NAME_HERE>"
    #           send_resolved: true
    #           title: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ end }}"
    #           text: "{{ range .Alerts }}{{ .Annotations.description }}\n{{ end }}"

  # additionalPrometheusRulesMap:
  #   rule-name:
  #     groups:
  #     - name: emojivoto-instance-down
  #       rules:
  #         - alert: EmojivotoInstanceDown
  #           expr: sum(kube_pod_owner{namespace="emojivoto"}) by (namespace) < 4
  #           for: 1m
  #           labels:
  #             severity: 'critical'
  #             alert_type: 'infrastructure'
  #           annotations:
  #             description: ' The Number of pods from the namespace {{ $labels.namespace }} is lower than the expected 4. '
  #             summary: 'Pod in {{ $labels.namespace }} namespace down'

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: true
    adminPassword: prom-operator # Please change the default password in production !!!
  #   affinity:
  #     nodeAffinity:
  #       preferredDuringSchedulingIgnoredDuringExecution:
  #       - weight: 1
  #         preference:
  #           matchExpressions:
  #           - key: preferred
  #             operator: In
  #             values:
  #             - observability

    # # Starter Kit setup for DigitalOcean Block Storage
    # persistence:
    #   enabled: true
    #   storageClassName: do-block-storage
    #   accessModes: ["ReadWriteOnce"]
    #   size: 5Gi

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true

    ## Starter Kit components service monitors
    #
    # Uncomment the following section to enable emojivoto service monitoring
    additionalServiceMonitors:
      - name: "test-vllm-monitor2"
        selector:
          matchLabels:
            app.kubernetes.io/managed-by: Helm
            environment: test
            release: test
        namespaceSelector:
          matchNames: 
            - default
        endpoints:
          - port: "service-port"

    # # Uncomment the following section to enable ingress-nginx service monitoring
    #   - name: "ingress-nginx-monitor"
    #     selector:
    #       matchLabels:
    #         app.kubernetes.io/name: ingress-nginx
    #     namespaceSelector:
    #       matchNames:
    #         - ingress-nginx
    #     endpoints:
    #       - port: "metrics"

    # # Uncomment the following section to enable Loki service monitoring
    #   - name: "loki-monitor"
    #     selector:
    #       matchLabels:
    #         app: loki
    #         release: loki
    #     namespaceSelector:
    #       matchNames:
    #         - loki-stack
    #     endpoints:
    #       - port: "http-metrics"

    # # Uncomment the following section to enable Promtail service monitoring
    #   - name: "promtail-monitor"
    #     selector:
    #       matchLabels:
    #         app: promtail
    #         release: loki
    #     namespaceSelector:
    #       matchNames:
    #         - loki-stack
    #     endpoints:
    #       - port: "http-metrics"

    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/storage.md
    ##
    # prometheusSpec:
    #   affinity:
    #     nodeAffinity:
    #       preferredDuringSchedulingIgnoredDuringExecution:
    #       - weight: 1
    #         preference:
    #           matchExpressions:
    #           - key: preferred
    #             operator: In
    #             values:
    #             - observability
      # storageSpec:
      #   volumeClaimTemplate:
      #     spec:
      #       storageClassName: do-block-storage
      #       accessModes: ["ReadWriteOnce"]
      #       resources:
      #         requests:
      #           storage: 5Gi
